# -*- coding: utf-8 -*-
"""YouTube Views Random Forest Regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1Jj-t9j__nBVTCq5RN63VLACgUbSRuCJl
"""

!pip install catboost

import pandas as pd
import numpy as np
from textblob import TextBlob
from sklearn.model_selection import train_test_split
from catboost import CatBoostRegressor, Pool
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score
import matplotlib.pyplot as plt

# Load dataset
df = pd.read_csv('USvideos.csv')

df.shape[1]

# Keep last snapshot per video by max views
idx = df.groupby('video_id')['views'].idxmax()
df_grouped = df.loc[idx].reset_index(drop=True)

# Feature engineering
df_grouped['title_len'] = df_grouped['title'].apply(len)
df_grouped['title_word_count'] = df_grouped['title'].apply(lambda x: len(str(x).split()))
df_grouped['title_sentiment'] = df_grouped['title'].apply(lambda x: TextBlob(str(x)).sentiment.polarity)

df_grouped['description'].fillna('', inplace=True)
df_grouped['description_len'] = df_grouped['description'].apply(len)
df_grouped['description_word_count'] = df_grouped['description'].apply(lambda x: len(str(x).split()))
df_grouped['description_sentiment'] = df_grouped['description'].apply(lambda x: TextBlob(str(x)).sentiment.polarity)

df_grouped['tag_count'] = df_grouped['tags'].apply(lambda x: len(x.split('|')))\

df_grouped['publish_hour'] = pd.to_datetime(df_grouped['publish_time']).dt.hour

df_grouped['trending_date'] = pd.to_datetime(df_grouped['trending_date'], format='%y.%d.%m')
df_grouped['trending_dayofweek'] = df_grouped['trending_date'].dt.dayofweek
df_grouped['trending_month'] = df_grouped['trending_date'].dt.month
df_grouped['trending_year'] = df_grouped['trending_date'].dt.year

channel_counts = df_grouped['channel_title'].value_counts().to_dict()
df_grouped['channel_video_count'] = df_grouped['channel_title'].map(channel_counts)

# Drop columns
df_grouped = df_grouped.drop(columns=['video_id', 'trending_date', 'thumbnail_link', 'publish_time', 'tags', 'title', 'description', 'likes', 'dislikes', 'comment_count'])

df_grouped['views_log'] = np.log1p(df_grouped['views'])

df_grouped

# Prepare features and target
X = df_grouped.drop(columns=['views', 'views_log'])
y = df_grouped['views_log']

cat_features = ['channel_title']

# Train Random Forest
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=1)

reg = CatBoostRegressor(
    iterations=500,
    depth=6,
    learning_rate=0.1,
    random_seed=1,
    verbose=50
)

train_pool = Pool(X_train, y_train, cat_features=cat_features)
test_pool = Pool(X_test, y_test, cat_features=cat_features)

reg.fit(train_pool, eval_set=test_pool)

# Predict
y_pred = reg.predict(X_test)

y_pred_normal = np.expm1(y_pred)
y_test_normal = np.expm1(y_test)

# Error
print(f"Mean Squared Error: {mean_squared_error(y_test_normal, y_pred_normal):.2f}")
print(f"Mean Absolute Error: {mean_absolute_error(y_test_normal, y_pred_normal):.2f}")
print(f"R^2 Score: {r2_score(y_test_normal, y_pred_normal):.2f}")

# Feature Importance
feat_importances = pd.Series(reg.feature_importances_, index=X.columns)
feat_importances.sort_values().plot(kind='barh')
plt.title('Feature Importances')
plt.show()